{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1737,
     "status": "ok",
     "timestamp": 1615226566432,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "qZYoaqIq4Fqs",
    "outputId": "553c9e2d-d5fb-47ec-a47a-2e2abd5c4aec"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount(r'C:\\Users\\vinee\\Desktop\\Mini Project 2B (ITM601)\\awareness-of-waste-recycling-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2632,
     "status": "ok",
     "timestamp": 1615226567340,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "1RJwB7DO4T0W",
    "outputId": "e34fedcc-1428-4f5b-8013-fb8d44ed4521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinee\\Desktop\\Mini Project 2B (ITM601)\\awareness-of-waste-recycling-master\n"
     ]
    }
   ],
   "source": [
    "%cd \"C:\\Users\\vinee\\Desktop\\Mini Project 2B (ITM601)\\awareness-of-waste-recycling-master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2626,
     "status": "ok",
     "timestamp": 1615226567343,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "YMJt9rlW4ixX"
   },
   "outputs": [],
   "source": [
    "# !unzip recycling\\ waste.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5945,
     "status": "ok",
     "timestamp": 1615226570671,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "y1uIVGqD4p27"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5939,
     "status": "ok",
     "timestamp": 1615226570672,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "73cocERU6Jpt",
    "outputId": "4ac46cb8-62b3-4442-b5d8-23afc5b03b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8369 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "# let's prepare the data and generate the data\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "gen_train = ImageDataGenerator(rescale = 1/255, shear_range = 0.2, zoom_range = 0.2, \n",
    "                               brightness_range = (0.1, 0.5), horizontal_flip=True)\n",
    "\n",
    "train_data = gen_train.flow_from_directory(r\"C:\\Users\\vinee\\Desktop\\Mini Project 2B (ITM601)\\awareness-of-waste-recycling-master\\recycling waste\",\n",
    "                                           target_size = (224, 224), batch_size = 32, class_mode=\"categorical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5931,
     "status": "ok",
     "timestamp": 1615226570673,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "v7E5kRJl8BwH"
   },
   "outputs": [],
   "source": [
    "# let's create a model\n",
    "# here i'm going to use VGG16 model's parameter to solve this problem\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "# here i'm going to take input shape, weights and bias from imagenet and include top False means\n",
    "# i want to add input, flatten and output layer by my self\n",
    "\n",
    "vgg16 = VGG16(input_shape = (224, 224, 3), weights = \"imagenet\", include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5924,
     "status": "ok",
     "timestamp": 1615226570674,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "U5v7MH4-90yI"
   },
   "outputs": [],
   "source": [
    "# now vgg16 weights are already train so i don't want to train that weights again\n",
    "# so let's make trainable = False\n",
    "\n",
    "for layer in vgg16.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5918,
     "status": "ok",
     "timestamp": 1615226570675,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "v998AH3K-Dga"
   },
   "outputs": [],
   "source": [
    "# let's add flatten layer or let's connect VGG16 with our own flatten layer\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "x = layers.Flatten()(vgg16.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5911,
     "status": "ok",
     "timestamp": 1615226570676,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "VlXSZUNA-Rrb",
    "outputId": "a742700d-8123-49fb-aafe-e2bd1c52ae8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 9)                 225801    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,940,489\n",
      "Trainable params: 225,801\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinee\\AppData\\Local\\Temp/ipykernel_32764/2812959610.py:12: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  result = model.fit_generator(train_data, epochs = 28, steps_per_epoch=len(train_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28\n",
      "262/262 [==============================] - 916s 3s/step - loss: 1.9028 - accuracy: 0.3447\n",
      "Epoch 2/28\n",
      "262/262 [==============================] - 1110s 4s/step - loss: 1.5649 - accuracy: 0.4622\n",
      "Epoch 3/28\n",
      "262/262 [==============================] - 22734s 87s/step - loss: 1.4895 - accuracy: 0.4943\n",
      "Epoch 4/28\n",
      "262/262 [==============================] - 994s 4s/step - loss: 1.4012 - accuracy: 0.5160\n",
      "Epoch 5/28\n",
      "262/262 [==============================] - 1136s 4s/step - loss: 1.3587 - accuracy: 0.5327\n",
      "Epoch 6/28\n",
      "262/262 [==============================] - 1163s 4s/step - loss: 1.2618 - accuracy: 0.5695\n",
      "Epoch 7/28\n",
      "262/262 [==============================] - 1596s 6s/step - loss: 1.2710 - accuracy: 0.5737\n",
      "Epoch 8/28\n",
      "262/262 [==============================] - 1174s 4s/step - loss: 1.2585 - accuracy: 0.5718\n",
      "Epoch 9/28\n",
      "262/262 [==============================] - 999s 4s/step - loss: 1.1386 - accuracy: 0.6131\n",
      "Epoch 10/28\n",
      "262/262 [==============================] - 955s 4s/step - loss: 1.1459 - accuracy: 0.6057\n",
      "Epoch 11/28\n",
      "221/262 [========================>.....] - ETA: 1:35:43 - loss: 1.1347 - accuracy: 0.6120"
     ]
    }
   ],
   "source": [
    "# now let's add output layers or prediction layer\n",
    "\n",
    "prediction = layers.Dense(units = 9, activation=\"softmax\")(x)\n",
    "\n",
    "# creating a model object\n",
    "\n",
    "model = tf.keras.models.Model(inputs = vgg16.input, outputs=prediction)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics =[\"accuracy\"])\n",
    "\n",
    "result = model.fit_generator(train_data, epochs = 28, steps_per_epoch=len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4484071,
     "status": "ok",
     "timestamp": 1615231048857,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "xiJfS-Wf-l2r",
    "outputId": "9f72f0c7-4883-4edd-f2b4-5faaee004246"
   },
   "outputs": [],
   "source": [
    "# now let's compile the model\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics =[\"accuracy\"])\n",
    "\n",
    "result = model.fit_generator(train_data, epochs = 28, steps_per_epoch=len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4484067,
     "status": "ok",
     "timestamp": 1615231048863,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "x9ODKg5O_F4-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "output_class = [\"batteries\", \"clothes\", \"e-waste\", \"glass\", \"light blubs\", \"metal\", \"organic\", \"paper\", \"plastic\"]\n",
    "def waste_prediction(new_image):\n",
    "  test_image = image.load_img(new_image, target_size = (224,224))\n",
    "  plt.axis(\"off\")\n",
    "  plt.imshow(test_image)\n",
    "  plt.show()\n",
    " \n",
    "  test_image = image.img_to_array(test_image) / 255\n",
    "  test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "  predicted_array = model.predict(test_image)\n",
    "  predicted_value = output_class[np.argmax(predicted_array)]\n",
    "  predicted_accuracy = round(np.max(predicted_array) * 100, 2)\n",
    "\n",
    "  print(\"Your waste material is \", predicted_value, \" with \", predicted_accuracy, \" % accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 4485968,
     "status": "ok",
     "timestamp": 1615231050772,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "tkwu-yylg73e",
    "outputId": "6b9b722b-48cf-4f69-f9d5-1cb5fec4dd76"
   },
   "outputs": [],
   "source": [
    "waste_prediction(\"toy.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 4485962,
     "status": "ok",
     "timestamp": 1615231050775,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "KPEbBHaivBKR",
    "outputId": "72875b18-ce11-4608-a65f-db29847f6ca1"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Accuracy\")\n",
    "plt.plot(result.history[\"accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 4485958,
     "status": "ok",
     "timestamp": 1615231050779,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "n8nWUraqwUZ4",
    "outputId": "07ac87c5-2fda-4a6a-c554-c8bbe64e2fc2"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Loss\")\n",
    "plt.plot(result.history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4485954,
     "status": "ok",
     "timestamp": 1615231050782,
     "user": {
      "displayName": "Jay Soni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhI6xgbQk71BxJYiUo2i9IM4h6o-7df9LgCTlUrAg=s64",
      "userId": "17999048048998522192"
     },
     "user_tz": -330
    },
    "id": "C6bfVNY5u7bB"
   },
   "outputs": [],
   "source": [
    "model.save(\"classifyWaste.h5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOkx2ycqImBl0lIiPU0h2iS",
   "collapsed_sections": [],
   "name": "waste_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
